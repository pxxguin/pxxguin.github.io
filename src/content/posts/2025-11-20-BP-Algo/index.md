---
title: Can you explain about Back Propagation?
published: 2025-11-21
description: DeepLì—ì„œ ì¤‘ì‹¬ì´ ë˜ëŠ” ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìžˆë‚˜ìš”?
tags: [DeepL]
category: DeepL
draft: true
---

### ðŸ¤¦ðŸ»â€â™‚ï¸ ë“¤ì–´ê°€ê¸°ì— ì•žì„œ...
> ë„¤. ì´ë²ˆì—ëŠ” DeepLì—ì„œ ì œê°€ ì œì¼ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ë¶€ë¶„ ì¤‘ ì²«ë²ˆì§¸ ë¶€ë¶„ìž…ë‹ˆë‹¤. ìš°ë¦¬ê°€ DeepL ëª¨ë¸ì„ ì™œ ì‚¬ìš©í•˜ë‚˜ìš”? í•´ê²°í•˜ê³  ì‹¶ì€ downstream workê°€ ìžˆì„ ë•Œ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì—ëŠ” ê°€ì¤‘ì¹˜(Weight)ë¼ëŠ” ê°œë…ì´ ì¡´ìž¬í•˜ì£ . ì²˜ìŒì— ê°€ì¤‘ì¹˜ëŠ” ëžœë¤í•œ ê°’ìœ¼ë¡œ ë¶€ì—¬ë˜ì£ .(ë¬¼ë¡  Heê¸°ë²•ì´ë‚˜ Xavierê¸°ë²•ê³¼ ê°™ì´ íŠ¹ì • ë¶„í¬ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•˜ëŠ” ë°©ë²•ë„ ìžˆìŠµë‹ˆë‹¤.) ìš°ë¦¬ëŠ” ì´ ê°€ì¤‘ì¹˜ ê°’ì„ ê°€ì§„ ëª¨ë¸ì´, ìš°ë¦¬ê°€ ì²˜í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìžˆë„ë¡ ë§Œë“¤ì–´ì•¼í•©ë‹ˆë‹¤. ì´ëŠ” DeepLì˜ ëª©ì ì„ ë³´ì—¬ì£¼ëŠ”ë°, ê²°êµ­ ìµœì ì˜ ê°€ì¤‘ì¹˜ ê°’ì„ ì°¾ì•„ì•¼í•œë‹¤ëŠ”ê±°ì£ .
>
> ê·¸ëŸ¬ë©´, ì–´ë–»ê²Œ ìš°ë¦¬ëŠ” ìµœì ì˜ ê°€ì¤‘ì¹˜ ê°’ì„ ì°¾ì„ ìˆ˜ ìžˆì„ê¹Œìš”? ì´ë•Œ ë“±ìž¥í•˜ëŠ”ê²Œ Optimizerì¸ë° Optimzierì˜ ì¢…ë¥˜ì— ëŒ€í•´ì„œëŠ” ì•žìœ¼ë¡œ ì°¨ì°¨ ì–¸ê¸‰í•  ì˜ˆì •ì´ë¯€ë¡œ ì¼ë‹¨ ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì„ ì ìš©ì‹œì¼œì„œ ì—­ì „íŒŒ ê³¼ì •ì— ëŒ€í•´ì„œ ì´í•´í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

:::warning
ì§ìž‘í•˜ì…¨ë“¯ì´ ì´ë²ˆì— ì„¤ëª…í•  ë‚´ìš©ì€ ìˆ˜í•™ì  ë‚´ìš©ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤. ìˆ˜í•™ì´ë¼ê³  í•´ë´ì•¼ ë¯¸ë¶„, íŽ¸ë¯¸ë¶„ ë°–ì— ì—†ìœ¼ë‹ˆ ì£¼ì˜í•´ì£¼ì„¸ìš”!
:::

### ë¯¸ë¶„(Derivative)ì´ëž€?
ì´ê±´ ì¤‘í•™êµë•Œë¶€í„° ë°°ì› ë˜ ê°œë…ì´ë‹ˆê¹ ê°„ë‹¨ížˆ ì§šê³  ë„˜ì–´ê°ˆê²Œìš”. $\frac{\Delta f}{\Delta x}$ëŠ” $\frac{fë³€í™”ëŸ‰}{xë³€í™”ëŸ‰}$ìœ¼ë¡œ ë³¼ ìˆ˜ ìžˆëŠ”ê±°ì£ . 
:::tip
$f(x)=3x^2$ì„ ë¯¸ë¶„í•˜ë©´ $f'(x)=6x$ê°€ ë©ë‹ˆë‹¤.
:::

### íŽ¸ë¯¸ë¶„(Partial Derivative)ì´ëž€?
íŽ¸ë¯¸ë¶„ ê°œë…ì€ ë¹ ë¥´ë©´ ê³ ë“±í•™êµë•Œ, ì•„ë‹ˆë©´ ëŒ€í•™êµ 1í•™ë…„ë•Œ ë°°ìš°ëŠ” ê°œë…ì¸ë°ìš”. ë¯¸ë¶„ì˜ ê²½ìš° í•˜ë‚˜ì˜ ë‹¨ë³€ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„ì„ ì§„í–‰í–ˆë‹¤ë©´, íŽ¸ë¯¸ë¶„ì€ ë‹¤ë³€ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„ì„ ì§„í–‰í•œë‹¤ê³  ì´í•´í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
:::tip
$f(x, y)=x+y$ë¼ëŠ” ì‹ì´ ìžˆì„ ë•Œ, $\frac{\partial f(x, y)}{\partial x}=y$ì™€ $$\frac{\partial f(x, y)}{\partial y}=x$$ë¡œ ê³„ì‚°ë  ìˆ˜ ìžˆë‹¤ëŠ” ë¶€ë¶„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
$x$ì— ëŒ€í•œ ë¯¸ë¶„ì„ ì§„í–‰í•œë‹¤ë©´, $x$ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ì€ ìƒìˆ˜ì·¨ê¸‰ì„ í•œ ìƒíƒœì—ì„œ ë¯¸ë¶„ì„ í•œë‹¤ê³  ì´í•´í•˜ì‹œë©´ íŽ¸í•©ë‹ˆë‹¤.
:::

### ê²½ì‚¬í•˜ê°•ë²•ì˜ ê¸°ë³¸ì ì¸ ì´í•´
ê²½ì‚¬í•˜ê°•ë²•.. ì–´ë ¤ì›Œ ë³´ì´ëŠ” ê°œë…ì´ì§€ë§Œ ì´ ë¶€ë¶„ì€ Optimizer ë¶€ë¶„ì—ì„œ ë” ìžì„¸ížˆ í’€ì–´ì“¸ê²ë‹ˆë‹¤. ì§€ê¸ˆì€ ì´ëŸ°ê²Œ ìžˆë‹¤ ì •ë„ë¡œë§Œ ì•Œì•„ë‘ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.
> $$W_{t+1}=W_{t}-\eta \cdot \frac{\partial L}{\partial w}$$

ë³€ìˆ˜ í•˜ë‚˜ì”© ì„¤ëª…í• ê²Œìš”. $W_{t+1}$ì€ ë‹¤ìŒ ê°€ì¤‘ì¹˜, $W_t$ëŠ” í˜„ìž¬ ê°€ì¤‘ì¹˜, $\eta$ëŠ” í•™ìŠµë¥ (Learning Rate, lr), $\frac{\partial L}{\partial w}$ëŠ” wì˜ ë³€í™”ì— ë”°ë¥¸ ì†ì‹¤í•¨ìˆ˜ì˜ ë³€í™”ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  
ë”°ë¼ì„œ ìœ„ ìˆ˜ì‹ì€, ì—­ì „íŒŒë¥¼ ì§„í–‰í•œ ê°’ì„ ì´ìš©í•˜ì—¬ ë‹¤ìŒ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤ê³  ì´í•´í•˜ì‹œë©´ íŽ¸í•©ë‹ˆë‹¤.

### ì—­ì „íŒŒì˜ ì—°ì‚° ê³¼ì • 1
ì›ëž˜ì˜ ì‹ ê²½ë§ì—ì„œëŠ” ë¬´ ì¡° ê±´ í™œì„±í™” í•¨ìˆ˜(Activation Function, $\tau$)ê°€ ìžˆì–´ì•¼í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ˆë°˜ë¶€í„° í™œì„±í™” í•¨ìˆ˜ë¥¼ ë„£ì€ ìƒíƒœë¡œ ì—­ì „íŒŒ ì—°ì‚°ì„ í•˜ê²Œ ë˜ë©´ ì˜ˆ.. ë³µìž¡í•´ì§‘ë‹ˆë‹¤. $Sigmoid$ì™€ $Tanh$ê°™ì€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•´ì•¼í•˜ëŠ”ë° ì¼ë‹¨ì€ ì—†ëŠ” ìƒí™©ì—ì„œ í•œë²ˆ ë³´ìžêµ¬ìš”.

![normal](./Normal_back.png)

> ì˜ˆì‹œë¥¼ í•˜ë‚˜ ë“¤ì–´ë´…ì‹œë‹¤. ìš°ë¦¬ê°€ ê³¼ì¼ ê°€ê³„ì—ì„œ ì‚¬ê³¼ë¥¼ ì‚´ê²ë‹ˆë‹¤. ì‚¬ê³¼ì˜ ê°€ê²©ì€ 100ì›ì´ê³  ì´ 2ê°œ ì‚´ê²ë‹ˆë‹¤. ìš°ë¦¬ê°€ ë¬¼ê±´ì„ ì‚¬ë©´ ì†Œë¹„ì„¸ë¥¼ ë‚´ì•¼ê² ì£ ? ì´ ì†Œë¹„ì„¸ê°€ 1.1ì´ë¼ê³  í•©ì‹œë‹¤. ì €ëŠ” ì œê°€ ë‚´ì•¼í•  ëˆì´ ì‚¬ê³¼ì˜ ê°œìˆ˜, ì‚¬ê³¼ì˜ ê°€ê²©, ì†Œë¹„ì„¸ ì¤‘ì—ì„œ ì–´ë–¤ ê°’ì´ ê°€ê²© ë³€ë™ì— ë¯¼ê°í•œì§€ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë¨¼ì € A, Bë¥¼ ì°¨ë¡€ëŒ€ë¡œ êµ¬í•´ë³¼ê¹Œìš”? (ê³„ì‚°í•˜ê¸° ì•žì„œì„œ ê° ë³€ìˆ˜ë“¤ì„ ë¬¸ìžë¡œ ì •ì˜í• ê²Œìš”.)
>
> ë¨¼ì € $ì‚¬ê³¼ ê°€ê²©:=x$, $ì‚¬ê³¼ ê°œìˆ˜:=y$, $ì†Œë¹„ì„¸:=z$ë¡œ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.
>
> $A=x \cdot y$ì´ ì²«ë²ˆì§¸ ì‹ì´ ë˜ê² ê³  $B=A \cdot z$ê°€ ë‘ë²ˆì§¸ ì‹ì´ ë˜ê² ë„¤ìš”.
>
> 

